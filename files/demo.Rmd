---
title: "R Demo"
author: "Bingxin Qi"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

### Prepare for analyses

```{r message = FALSE, warning = FALSE, paged.print = FALSE}
library(dplyr)
library(car)
library(kableExtra)
library(tidyverse)
library(caret)
library(gmodels)
library(e1071)
library(knitr)
library(ggmap)
```

### Load data

```{r, echo=FALSE}
d_force <- read.csv(url("https://cpe-analyst-homework-assignment.s3.us-east-2.amazonaws.com/analyst_assignment_data_files/Police_Use_of_Force.csv"))
d_neigh <- read.csv(url("https://cpe-analyst-homework-assignment.s3.us-east-2.amazonaws.com/analyst_assignment_data_files/NEIGHBORHOOD_CRIME_STATS.csv"))
d_poli_stop <- read.csv(url("https://cpe-analyst-homework-assignment.s3.us-east-2.amazonaws.com/analyst_assignment_data_files/Police_Stop_Data.csv"))
d_pede_stop <- read.csv(url("https://cpe-analyst-homework-assignment.s3.us-east-2.amazonaws.com/analyst_assignment_data_files/pedestrian_stops.csv"))
```

## **Part 1: Data Wrangling and Summarization**

First, remove duplicates.

```{r}
d_force2 <-
  d_force[!duplicated(d_force[c("CaseNumber", "SubjectRoleNumber")]), ]
```

Second, count unique values.

```{r}
nrow(d_force2)
```

Remove irrelevant data.

```{r}
invalid <-
  c(
    "GOA-Gone on Arrival",
    "AOK- All OK",
    "UTL-Unable to Locate",
    "TOW-Towed",
    "SEC- Secured",
    "CNL-Cancel",
    "UNF-Unfounded",
    "NOS-No Service",
    "AQT-All Quiet",
    "FTC-Fail to Clear",
    "FAL-False"
  )
d_poli_stop2 <- filter(d_poli_stop,!(callDisposition %in% invalid))
```

Then create dataframe for vehicle.

```{r}
v_p <- c("Traffic Law Enforcement (P)", "Suspicious Vehicle (P)")
v_stops <- filter(d_poli_stop2, (problem %in% v_p))
```

Third, create dataframe for pedestrians.

```{r}
p_p <- c("Suspicious Person (P)")
# Pedestrian stops
p_stops <- filter(d_poli_stop2, (problem %in% p_p))
```

Reclassify values (for race attribute)

```{r}
v_stops$race <-
  recode(v_stops$race,
         "c('','Other','Unknown')='Other';c('East African')='Black'")
p_stops$race <-
  recode(p_stops$race,
         "c('','Other','Unknown')='Other';c('East African')='Black'")
```

Calculate total number and percent by race.

```{r}
VSgp <-
  v_stops %>% group_by(race) %>% summarise(total_vehicle_stops = n(),
                                           percent_vehicle_stops = n() / nrow(.))
VSgp2 <-
  v_stops %>% group_by(race) %>% summarise(vehicle_percent_search = sum(personSearch ==
                                                                          "YES" | vehicleSearch == "YES") / nrow(.))
sumVP <- merge(VSgp, VSgp2)
PSgp <-
  p_stops %>% group_by(race) %>% summarise(
    total_pedestrian_stops = n(),
    percent_pedestrian_stops = n() / nrow(.)
  )
PSgp2 <-
  p_stops %>% group_by(race) %>% summarise(pedestrian_percent_search = sum(personSearch ==
                                                                             "YES" | vehicleSearch == "YES") / nrow(.))
sumVP2 <- merge(PSgp, PSgp2)
sumVP <- merge(sumVP, sumVP2)
race <-
  c('White', 'Black', 'Latino', "Asian", "Native American", "Other")
percent_pop <- c(0.624, 0.169, 0.091, 0.06, 0.009, 0.047)
df_pop <- data.frame(race, percent_pop)
sumVP <- merge(sumVP, df_pop)
```

Produce a table to summarize results.

```{r}
colnames(sumVP) <-
  c(
    "Race",
    "Total Stops",
    "Percent of Stops",
    "Percent of Search",
    "Total Stops",
    "Percent of Stops",
    "Percent of Search",
    "Percent of Population"
  )

kbl(sumVP) %>%
  kable_classic() %>%
  add_header_above(c(
    " " = 1,
    "Vehicle" = 3,
    "Pedestrian" = 3,
    "Minneapolis Population" = 1
  ))
```

## **Part 2: Data Visualization**

First, filter data by removing missing values and select data.

```{r}
d_neigh[["number"]][is.na(d_neigh[["number"]])] <- 0
d_neigh$neighborhood = toupper(d_neigh$neighborhood)

neigh <-
  c("CEDAR RIVERSIDE",
    "DOWNTOWN WEST",
    "LYNDALE",
    "SEWARD",
    "NEAR - NORTH")
crime <- c("Auto Theft", "Arson")
d_neigh2 <- filter(d_neigh, !(ucrDescription %in% crime))
```

Second, summarize results.

```{r}
d_neigh_ct <-
  d_neigh2 %>% group_by(ucrDescription, neighborhood) %>% summarise(Count_Neightborhood =
                                                                      sum(number))
d_neigh_tot <-
  d_neigh2 %>% group_by(ucrDescription) %>% summarise(Count_Total = sum(number))
d_neigh_ctAll <-
  merge(d_neigh_ct, d_neigh_tot, by = "ucrDescription")
d_neigh_ctAll[, "Percent"] <-
  d_neigh_ctAll[, "Count_Neightborhood"] / d_neigh_ctAll[, "Count_Total"]
d_neigh3 <- filter(d_neigh_ctAll, (neighborhood %in% neigh))
```

Third, produce visualization.

```{r fig.width=10, fig.height=6}
p <-
  ggplot(d_neigh3, aes(ucrDescription, Percent, fill = neighborhood)) + geom_bar(position = "dodge",
                                                                                 width = 0.5,
                                                                                 stat = "identity")
p + labs(fill = "Neighborhood",
         x = "Crime",
         y = "Percent of Total Incidents",
         title = "Neighborhood Crime Summary") + theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15),  plot.title = element_text(size = 18, lineheight = 0.9, hjust = 0.5)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1L))
```

## **Question 3: Statistical Modelling**

### Build Binary Logistic Regression Model

```{r}
d_pede_stop2 <-
  d_pede_stop[, c("SUBJECT_RACE", "SUBJECT_GENDER", "SEARCH_CONDUCTED")]
d_pede_stop2 = d_pede_stop2[]
d_pede_stop2$SEARCH_CONDUCTED <-
  as.factor(d_pede_stop2$SEARCH_CONDUCTED)
logistic_model <-
  glm(SEARCH_CONDUCTED ~ SUBJECT_RACE + SUBJECT_GENDER,
      family = binomial(),
      d_pede_stop2)
summary(logistic_model)
```

### Make Predictions and Preview Results

In order to make prediction with the logistic model, a threshold value is selected.

```{r}
table(d_pede_stop$SEARCH_CONDUCTED)
```

Therefore, based on the threshold, the model is constructed as follow:

```{r}
pred_prob <- predict(logistic_model, d_pede_stop2, type = "response")
d_pede_stop2$pred_class <-
  ifelse(logistic_model$fitted.values >= 4130 / (4130 + 6883), "YES", "NO")
cResult <-
  table(d_pede_stop2$SEARCH_CONDUCTED, d_pede_stop2$pred_class)
cResult
```

### Build Confusion Matrix for Evaluation

```{r}
d_pede_stop2$stop_vp <- as.factor(d_pede_stop2$SEARCH_CONDUCTED)
d_pede_stop2$pred_class <- as.factor(d_pede_stop2$pred_class)
expected_value <- d_pede_stop2$SEARCH_CONDUCTED
predicted_value <- d_pede_stop2$pred_class
CrossTable(expected_value, predicted_value)
cm <- confusionMatrix(expected_value, predicted_value)
cm
```

### Summarize Performance in Tabular Format

```{r}
draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1, 1, 2)))
  par(mar = c(2, 2, 2, 2))
  plot(
    c(100, 345),
    c(300, 450),
    type = "n",
    xlab = "",
    ylab = "",
    xaxt = 'n',
    yaxt = 'n'
  )
  title('Confusion Matrix Results', cex.main = 2)
  
  rect(150, 430, 240, 370, col = '#99c2ff')
  text(195, 435, 'Not Searched', cex = 1.2)
  rect(250, 430, 340, 370, col = '#ffaa80')
  text(295, 435, 'Searched', cex = 1.2)
  text(125,
       370,
       'Expected',
       cex = 1.3,
       srt = 90,
       font = 2)
  text(245, 450, 'Predicted', cex = 1.3, font = 2)
  rect(150, 305, 240, 365, col = '#ffaa80')
  rect(250, 305, 340, 365, col = '#99c2ff')
  text(140, 400, 'Not Searched', cex = 1.2, srt = 90)
  text(140, 335, 'Searched', cex = 1.2, srt = 90)
  
  res <- as.numeric(cm$table)
  text(195,
       400,
       res[1],
       cex = 1.6,
       font = 2,
       col = 'white')
  text(195,
       335,
       res[2],
       cex = 1.6,
       font = 2,
       col = 'white')
  text(295,
       400,
       res[3],
       cex = 1.6,
       font = 2,
       col = 'white')
  text(295,
       335,
       res[4],
       cex = 1.6,
       font = 2,
       col = 'white')
  
  plot(
    c(100, 0),
    c(100, 0),
    type = "n",
    xlab = "",
    ylab = "",
    main = "DETAILS",
    xaxt = 'n',
    yaxt = 'n'
  )
  text(10, 85, names(cm$byClass[1]), cex = 1.2, font = 2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex = 1.2)
  text(30, 85, names(cm$byClass[2]), cex = 1.2, font = 2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex = 1.2)
  text(50, 85, names(cm$byClass[5]), cex = 1.2, font = 2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex = 1.2)
  text(70, 85, names(cm$byClass[6]), cex = 1.2, font = 2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex = 1.2)
  text(90, 85, names(cm$byClass[7]), cex = 1.2, font = 2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex = 1.2)
  
  text(30, 35, names(cm$overall[1]), cex = 1.5, font = 2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex = 1.4)
  text(70, 35, names(cm$overall[2]), cex = 1.5, font = 2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex = 1.4)
}

draw_confusion_matrix(cm)
```

## Explore Patterns with Kernel Density Estimation

FIrst, remove data with missing location information and selected cases that the search is conducted.

```{r}
df_instance <-
  subset(d_pede_stop,
         LOCATION_LONGITUDE != 0 &
           LOCATION_LATITUDE != 0 & SEARCH_CONDUCTED == "YES")
```

### Plot Raw Data

```{r}
ggplot(df_instance, aes(x = LOCATION_LONGITUDE, y = LOCATION_LATITUDE)) +
  geom_point() +
  coord_equal() +
  xlab('Longitude') +
  ylab('Latitude')
```

## Spatial Hotpot Analysis - Kernel Density Estimation (KDE)

Kernel Density calculates the density of point features around each output raster cell. Output pixels with higher intensity values indicates regions with higher number of instances.

```{r}
df_white <- subset(df_instance, SUBJECT_RACE == "White")
ggplot(df_instance, aes(x = LOCATION_LONGITUDE, y = LOCATION_LATITUDE)) +
  coord_equal() +
  xlab('Longitude') +
  ylab('Latitude') +
  stat_density2d(
    aes(fill = ..level..),
    alpha = .5,
    h = .02,
    n = 300,
    geom = "polygon",
    data = df_white
  ) +
  scale_fill_viridis_c() +
  theme(legend.position = 'left')
```
